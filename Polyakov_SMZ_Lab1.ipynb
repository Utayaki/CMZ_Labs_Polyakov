{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a2611b-e0b4-44fe-86e6-a49f9c738cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3088129-59b0-4979-965a-a4b96a680960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BloatWareConv2D:\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "        padding_mode=\"zeros\",\n",
    "        dtype=None,\n",
    "    ):\n",
    "        padding_modes = [\"zeros\", \"reflect\", \"circular\"]\n",
    "        if padding_mode not in padding_modes:\n",
    "            raise ValueError(\"Invalid padding_mode\")\n",
    "        self.padding_mode = padding_mode\n",
    "\n",
    "        if isinstance(in_channels, int) and in_channels > 0:\n",
    "            self.in_channels = in_channels\n",
    "        else:\n",
    "            raise ValueError(\"Invalid in_channels\")\n",
    "\n",
    "        if isinstance(out_channels, int) and out_channels > 0:\n",
    "            self.out_channels = out_channels\n",
    "        else:\n",
    "            raise ValueError(\"Invalid out_channels\")\n",
    "\n",
    "        if isinstance(groups, int) and groups > 0:\n",
    "            self.groups = groups\n",
    "        else:\n",
    "            raise ValueError(\"Invalid groups\")\n",
    "\n",
    "        if isinstance(stride, int) and stride > 0:\n",
    "            self.stride = stride\n",
    "        else:\n",
    "            raise ValueError(\"Invalid stride\")\n",
    "\n",
    "        if isinstance(padding, int) and padding > -1:\n",
    "            self.padding = padding\n",
    "        else:\n",
    "            raise ValueError(\"Invalid padding\")\n",
    "\n",
    "        if isinstance(dilation, int) and dilation > 0:\n",
    "            self.dilation = dilation\n",
    "        else:\n",
    "            raise ValueError(\"Invalid dilation\")\n",
    "        if not (\n",
    "            (self.in_channels % self.groups == 0)\n",
    "            and (self.out_channels % self.groups == 0)\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"in_channels and out_channels must both be divisible by groups\"\n",
    "            )\n",
    "\n",
    "        # генерация bias\n",
    "        if bias == True:\n",
    "            self.bias = torch.rand(out_channels)\n",
    "        else:\n",
    "            self.bias = torch.zeros(out_channels)\n",
    "\n",
    "        # генерация ядра\n",
    "        if isinstance(kernel_size, tuple):\n",
    "            self.weight = torch.rand(\n",
    "                self.out_channels,\n",
    "                self.in_channels // self.groups,\n",
    "                kernel_size[0],\n",
    "                kernel_size[1],\n",
    "            )\n",
    "        elif isinstance(kernel_size, int):\n",
    "            self.weight = torch.rand(\n",
    "                self.out_channels,\n",
    "                self.in_channels // self.groups,\n",
    "                kernel_size,\n",
    "                kernel_size,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"kernel size must be int or tuple\")\n",
    "\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        if self.padding_mode == \"zeros\":\n",
    "            pad = torch.nn.ZeroPad2d(self.padding)\n",
    "            input_tensor = pad(input_tensor)\n",
    "        if self.padding_mode == \"reflect\":\n",
    "            pad = torch.nn.ReflectionPad2d(self.padding)\n",
    "            input_tensor = pad(input_tensor)\n",
    "        if self.padding_mode == \"circular\":\n",
    "            pad = torch.nn.CircularPad2d(self.padding)\n",
    "            input_tensor = pad(input_tensor)\n",
    "\n",
    "        result = []\n",
    "        for l in range(self.out_channels):\n",
    "            feature_map = np.array([])\n",
    "            for i in range(\n",
    "                0,\n",
    "                input_tensor.shape[1]\n",
    "                - ((self.weight.shape[2] - 1) * self.dilation + 1)\n",
    "                + 1,\n",
    "                self.stride,\n",
    "            ):  # (filter.size - 1)*dilation + 1 при delation\n",
    "                for j in range(\n",
    "                    0,\n",
    "                    input_tensor.shape[2]\n",
    "                    - ((self.weight.shape[3] - 1) * self.dilation + 1)\n",
    "                    + 1,\n",
    "                    self.stride,\n",
    "                ):\n",
    "                    all_channels_sum = 0\n",
    "                    for c in range(self.in_channels // self.groups):  # groups\n",
    "                        if self.groups > 1:\n",
    "                            val = input_tensor[\n",
    "                                l * (self.in_channels // self.groups) + c\n",
    "                            ][\n",
    "                                i: i\n",
    "                                + (self.weight.shape[2] - 1) * self.dilation\n",
    "                                + 1: self.dilation,\n",
    "                                j: j\n",
    "                                + (self.weight.shape[3] - 1) * self.dilation\n",
    "                                + 1: self.dilation,\n",
    "                            ]\n",
    "                        else:\n",
    "                            val = input_tensor[c][\n",
    "                                i: i\n",
    "                                + (self.weight.shape[2] - 1) * self.dilation\n",
    "                                + 1: self.dilation,\n",
    "                                j: j\n",
    "                                + (self.weight.shape[3] - 1) * self.dilation\n",
    "                                + 1: self.dilation,\n",
    "                            ]\n",
    "                        channel_sum = (val * self.weight[l][c]).sum()\n",
    "                        all_channels_sum += +channel_sum\n",
    "                    feature_map = np.append(\n",
    "                        feature_map, float(all_channels_sum + self.bias[l])\n",
    "                    )  # bias\n",
    "\n",
    "            result.append(\n",
    "                feature_map.reshape(\n",
    "                    (\n",
    "                        input_tensor.shape[1]\n",
    "                        - ((self.weight.shape[2] - 1) * self.dilation + 1)\n",
    "                    )\n",
    "                    // self.stride\n",
    "                    + 1,\n",
    "                    (\n",
    "                        input_tensor.shape[2]\n",
    "                        - ((self.weight.shape[3] - 1) * self.dilation + 1)\n",
    "                    )\n",
    "                    // self.stride\n",
    "                    + 1,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return np.array(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352da091-78c6-4655-ba14-fb5318cc240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchConv2D = torch.nn.Conv2d(3, 5, (3, 3), stride=1, padding=1, dilation=1)\n",
    "input_image = torch.randn(3, 50, 50)\n",
    "output = torchConv2D(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54c92c8a-c7e0-44fd-817b-9ad7e0b7a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "myConv2D = BloatWareConv2D(3, 5, (3, 3), stride=1, padding=1, dilation=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95c94cd6-4133-4eda-8864-6d16eb76adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "myConv2D.weight = torchConv2D.weight.detach().numpy()\n",
    "myConv2D.bias = torchConv2D.bias.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67b48c13-1671-45b6-ab17-6d0caa7a378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mock = myConv2D.forward(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73818bbf-97c9-4d51-babf-8c0c3741b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = output.detach().numpy().astype(\"float16\") == output_mock.astype(\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b60cf908-beb4-4923-98e6-747f1eba5edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e1fb5-7762-4802-ada5-cadc5d78908d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neyronki",
   "language": "python",
   "name": "neyronki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
