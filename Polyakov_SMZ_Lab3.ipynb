{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9526707e-c13e-41b9-8d6a-c0e289bbe035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "class BloatWareConvTranspose2D():\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, dtype=None):\n",
    "        \n",
    "        if isinstance(in_channels, int) and in_channels > 0:\n",
    "            self.in_channels = in_channels\n",
    "        else:\n",
    "            raise ValueError(\"Invalid in_channels\")\n",
    "        \n",
    "        if isinstance(out_channels, int) and out_channels > 0:\n",
    "            self.out_channels = out_channels\n",
    "        else:\n",
    "            raise ValueError(\"Invalid out_channels\")\n",
    "        \n",
    "        if isinstance(groups, int) and groups > 0:\n",
    "            self.groups = groups\n",
    "        else:\n",
    "            raise ValueError(\"Invalid groups\")\n",
    "\n",
    "        if isinstance(stride, int) and stride > 0:\n",
    "            self.stride = stride\n",
    "        else:\n",
    "            raise ValueError(\"Invalid stride\")\n",
    "        \n",
    "        if isinstance(padding, int) and padding > -1:\n",
    "            self.padding = padding\n",
    "        else:\n",
    "            raise ValueError(\"Invalid padding\")\n",
    "            \n",
    "        if isinstance(output_padding, int) and output_padding > -1:\n",
    "            self.output_padding = output_padding\n",
    "        else:\n",
    "            raise ValueError(\"Invalid output_padding\")\n",
    "            \n",
    "        if isinstance(dilation, int) and dilation > 0:\n",
    "            self.dilation = dilation\n",
    "        else:\n",
    "            raise ValueError(\"Invalid dilation\")\n",
    "            \n",
    "        if not((self.in_channels % self.groups == 0) and (self.out_channels % self.groups == 0)):\n",
    "            raise ValueError(\"in_channels and out_channels must both be divisible by groups\")\n",
    "        \n",
    "        if (self.output_padding >= self.dilation and self.output_padding >= self.stride) or (self.output_padding >= self.dilation and self.output_padding >= self.stride):\n",
    "            raise ValueError(\"output_padding should be smaller than dilation or stride\")\n",
    "\n",
    "        if bias == True:\n",
    "          self.bias = torch.rand(self.out_channels)\n",
    "        else:\n",
    "          self.bias = torch.zeros(self.out_channels)\n",
    "    \n",
    "        if isinstance(kernel_size, int):\n",
    "            self.weight = torch.rand(\n",
    "                self.in_channels,\n",
    "                self.out_channels,\n",
    "                kernel_size,\n",
    "                kernel_size,\n",
    "            )\n",
    "            self.kernel_size = kernel_size\n",
    "        else:\n",
    "            raise ValueError(\"kernel size must be int or tuple\")\n",
    "\n",
    "        self.dtype = dtype\n",
    "    \n",
    "    def forward(self, input_tensor):\n",
    "        result = []\n",
    "\n",
    "        for l in range(self.out_channels):\n",
    "    \n",
    "          feature_map = torch.zeros((input_tensor.shape[1]-1)*self.stride + self.dilation * (self.kernel_size-1)+1, (input_tensor.shape[2]-1)*self.stride  + self.dilation * (self.kernel_size-1)+1 ) #генерация пустой feature-map\n",
    "          for c in range (self.in_channels):\n",
    "    \n",
    "            for i in range (0, input_tensor.shape[1]):  #проход по всем пикселям изображения\n",
    "              for j in range (0, input_tensor.shape[2]):\n",
    "    \n",
    "                val = input_tensor[c][i][j]\n",
    "                proizv = val*self.weight[c][l]\n",
    "    \n",
    "                zero_tensor = torch.zeros((self.weight.shape[2]-1)*self.dilation+1, (self.weight.shape[3]-1)*self.dilation+1)\n",
    "    \n",
    "                for a in range (0, zero_tensor.shape[0], self.dilation):\n",
    "                  for b in range (0, zero_tensor.shape[1], self.dilation):\n",
    "                    zero_tensor[a][b] = proizv[a//self.dilation][b//self.dilation]\n",
    "    \n",
    "                res = np.add((zero_tensor), feature_map[i*self.stride:i*self.stride+(self.weight.shape[2]-1)*self.dilation+1, j*self.stride:j*self.stride+(self.weight.shape[3]-1)*self.dilation+1])\n",
    "                feature_map[i*self.stride:i*self.stride+(self.weight.shape[2]-1)*self.dilation+1, j*self.stride:j*self.stride+(self.weight.shape[3]-1)*self.dilation+1] = res\n",
    "    \n",
    "    \n",
    "          result.append(np.add(feature_map, np.full((feature_map.shape), self.bias[l])))\n",
    "    \n",
    "    \n",
    "        for t in range(len(result)):\n",
    "          if self.output_padding > 0:\n",
    "            pad_func = torch.nn.ConstantPad1d((0, self.output_padding, 0, self.output_padding), 0)\n",
    "            result[t] = pad_func(result[t])\n",
    "    \n",
    "          result[t] = result[t][0+self.padding:result[t].shape[0]-self.padding, 0+self.padding:result[t].shape[1]-self.padding]\n",
    "\n",
    "        return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4fde7dab-2430-42cc-b350-7635b35ddbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchConvTranspose2D = torch.nn.ConvTranspose2d(15, 5, 3, stride=1, padding=1, dilation=1)\n",
    "input_image = torch.randn(15, 50, 50)\n",
    "output = torchConvTranspose2D(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "16474b3d-b96f-4e76-9336-7cac7c3f5ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "myConvTranspose2D = BloatWareConvTranspose2D(15, 5, 3, stride=1, padding=1, dilation=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e17ce15d-53cf-46d0-9746-97fbc7d9e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "myConvTranspose2D.weight = torchConvTranspose2D.weight.detach().numpy()\n",
    "myConvTranspose2D.bias = torchConvTranspose2D.bias.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9e0bbbeb-41d2-4567-b608-4f0cb8f8a590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchConvTranspose2D.weight.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a29ff7d8-4cf8-42be-93b0-455344316d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mock = myConvTranspose2D.forward(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "da349a4f-cd9d-4a02-8d06-00f90d08d274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.32785314,  0.6804811 , -0.7160098 , ..., -0.03277095,\n",
       "         -2.111997  , -1.8800467 ],\n",
       "        [ 0.03005262, -1.025222  , -1.5796629 , ...,  1.5506848 ,\n",
       "         -0.5474681 , -0.189187  ],\n",
       "        [-0.4405887 , -0.72080725, -0.33276588, ...,  0.63289374,\n",
       "          1.134695  ,  1.291157  ],\n",
       "        ...,\n",
       "        [-1.1405551 ,  0.6169815 ,  0.0402675 , ...,  1.5148016 ,\n",
       "         -1.1224909 ,  0.33574766],\n",
       "        [ 0.26439023,  0.25257677,  0.6103852 , ...,  1.3353134 ,\n",
       "          0.30463696, -0.30290973],\n",
       "        [-0.08855709,  0.9013244 ,  0.5237863 , ..., -0.4800341 ,\n",
       "         -1.054776  , -0.5607661 ]],\n",
       "\n",
       "       [[-0.47302523, -0.70303595,  0.7516928 , ...,  1.4544461 ,\n",
       "          0.78200454,  0.75267035],\n",
       "        [-0.39958665,  0.15369278,  0.28326315, ...,  0.24692625,\n",
       "         -0.16545637, -0.32509685],\n",
       "        [-0.5051445 ,  2.0281272 , -0.05625352, ..., -0.6630189 ,\n",
       "         -0.90240747,  1.6585252 ],\n",
       "        ...,\n",
       "        [ 0.43847096,  0.25700855,  1.7181423 , ...,  1.2603476 ,\n",
       "         -0.31240162,  0.12657663],\n",
       "        [-0.53967905, -0.50677377, -1.6264749 , ...,  1.5853355 ,\n",
       "          0.884307  ,  0.13463369],\n",
       "        [-0.7899757 ,  1.1838877 , -0.22391638, ...,  0.90586406,\n",
       "         -0.5104531 , -0.378223  ]],\n",
       "\n",
       "       [[-0.7668174 ,  0.01883173,  0.64778066, ...,  0.22580259,\n",
       "          0.601709  ,  0.6250509 ],\n",
       "        [-1.3819326 , -0.28113148, -0.5020766 , ...,  1.3156304 ,\n",
       "         -2.585557  , -0.31955498],\n",
       "        [ 0.48414707, -0.59409344,  0.00853868, ..., -0.08627538,\n",
       "          0.5404728 ,  1.1222008 ],\n",
       "        ...,\n",
       "        [-0.20732532,  0.05181457, -0.10298941, ...,  1.0851344 ,\n",
       "         -0.69594765, -0.76817715],\n",
       "        [-0.33585858, -0.6752706 , -0.38137567, ..., -2.8024058 ,\n",
       "         -2.1720316 , -0.45379865],\n",
       "        [ 0.58317214, -1.3208816 , -0.5170985 , ..., -1.0795314 ,\n",
       "         -0.17337918,  0.07707943]],\n",
       "\n",
       "       [[ 0.21477349,  0.5167926 ,  0.22153072, ..., -0.27658337,\n",
       "          0.5989249 , -0.13414131],\n",
       "        [-0.14487904, -0.5923501 , -0.2969333 , ..., -0.23442309,\n",
       "          1.7161078 ,  0.18644652],\n",
       "        [ 0.3689953 , -3.5134704 ,  0.6079928 , ...,  0.5135044 ,\n",
       "          0.5579094 , -0.2014847 ],\n",
       "        ...,\n",
       "        [ 2.1535218 , -0.85433805,  0.49634516, ...,  0.46059984,\n",
       "         -0.09176451,  0.08318556],\n",
       "        [ 1.2374408 , -0.38622737, -0.55748147, ..., -0.6075881 ,\n",
       "         -1.0225276 , -0.09470078],\n",
       "        [-0.5013356 ,  1.0041956 ,  0.37917554, ..., -1.1607777 ,\n",
       "          0.10836732,  0.8726958 ]],\n",
       "\n",
       "       [[ 0.39462814, -1.7331067 , -1.8480312 , ..., -0.2651229 ,\n",
       "         -0.52631795, -0.67358136],\n",
       "        [-1.1325756 ,  0.69734836, -1.2574756 , ...,  0.03551496,\n",
       "          0.03971995,  0.11489981],\n",
       "        [-0.17930502,  2.635891  ,  1.1000944 , ..., -0.36787665,\n",
       "          0.5215673 ,  0.0471436 ],\n",
       "        ...,\n",
       "        [ 0.5176863 , -0.84682614,  0.37400094, ...,  2.2977645 ,\n",
       "         -1.1141735 , -0.03789843],\n",
       "        [ 0.10180894, -0.04159708, -0.8818321 , ...,  0.38629004,\n",
       "          0.8573907 , -0.08597644],\n",
       "        [ 0.3269538 ,  0.07847892, -1.02555   , ...,  0.14463782,\n",
       "          0.4106244 ,  0.2813702 ]]], dtype=float32)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "08cb2ac4-1c35-434e-803c-1fe27beec5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3279,  0.6805, -0.7160,  ..., -0.0328, -2.1120, -1.8800],\n",
       "         [ 0.0301, -1.0252, -1.5797,  ...,  1.5507, -0.5475, -0.1892],\n",
       "         [-0.4406, -0.7208, -0.3328,  ...,  0.6329,  1.1347,  1.2912],\n",
       "         ...,\n",
       "         [-1.1406,  0.6170,  0.0403,  ...,  1.5148, -1.1225,  0.3357],\n",
       "         [ 0.2644,  0.2526,  0.6104,  ...,  1.3353,  0.3046, -0.3029],\n",
       "         [-0.0886,  0.9013,  0.5238,  ..., -0.4800, -1.0548, -0.5608]],\n",
       "\n",
       "        [[-0.4730, -0.7030,  0.7517,  ...,  1.4544,  0.7820,  0.7527],\n",
       "         [-0.3996,  0.1537,  0.2833,  ...,  0.2469, -0.1655, -0.3251],\n",
       "         [-0.5051,  2.0281, -0.0563,  ..., -0.6630, -0.9024,  1.6585],\n",
       "         ...,\n",
       "         [ 0.4385,  0.2570,  1.7181,  ...,  1.2603, -0.3124,  0.1266],\n",
       "         [-0.5397, -0.5068, -1.6265,  ...,  1.5853,  0.8843,  0.1346],\n",
       "         [-0.7900,  1.1839, -0.2239,  ...,  0.9059, -0.5105, -0.3782]],\n",
       "\n",
       "        [[-0.7668,  0.0188,  0.6478,  ...,  0.2258,  0.6017,  0.6251],\n",
       "         [-1.3819, -0.2811, -0.5021,  ...,  1.3156, -2.5856, -0.3196],\n",
       "         [ 0.4841, -0.5941,  0.0085,  ..., -0.0863,  0.5405,  1.1222],\n",
       "         ...,\n",
       "         [-0.2073,  0.0518, -0.1030,  ...,  1.0851, -0.6959, -0.7682],\n",
       "         [-0.3359, -0.6753, -0.3814,  ..., -2.8024, -2.1720, -0.4538],\n",
       "         [ 0.5832, -1.3209, -0.5171,  ..., -1.0795, -0.1734,  0.0771]],\n",
       "\n",
       "        [[ 0.2148,  0.5168,  0.2215,  ..., -0.2766,  0.5989, -0.1341],\n",
       "         [-0.1449, -0.5924, -0.2969,  ..., -0.2344,  1.7161,  0.1864],\n",
       "         [ 0.3690, -3.5135,  0.6080,  ...,  0.5135,  0.5579, -0.2015],\n",
       "         ...,\n",
       "         [ 2.1535, -0.8543,  0.4963,  ...,  0.4606, -0.0918,  0.0832],\n",
       "         [ 1.2374, -0.3862, -0.5575,  ..., -0.6076, -1.0225, -0.0947],\n",
       "         [-0.5013,  1.0042,  0.3792,  ..., -1.1608,  0.1084,  0.8727]],\n",
       "\n",
       "        [[ 0.3946, -1.7331, -1.8480,  ..., -0.2651, -0.5263, -0.6736],\n",
       "         [-1.1326,  0.6973, -1.2575,  ...,  0.0355,  0.0397,  0.1149],\n",
       "         [-0.1793,  2.6359,  1.1001,  ..., -0.3679,  0.5216,  0.0471],\n",
       "         ...,\n",
       "         [ 0.5177, -0.8468,  0.3740,  ...,  2.2978, -1.1142, -0.0379],\n",
       "         [ 0.1018, -0.0416, -0.8818,  ...,  0.3863,  0.8574, -0.0860],\n",
       "         [ 0.3270,  0.0785, -1.0255,  ...,  0.1446,  0.4106,  0.2814]]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3f0996b2-c3e2-4d7b-a56a-91dbced6d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = output.detach().numpy().astype(\"float16\") == output_mock.astype(\n",
    "    \"float16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "250ef4e3-da99-4a24-a342-84384435b2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "59dc7d99-6609-4390-b8c3-0ea456ff4dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTranspConv2d(in_channels, out_channels, kernel_size, transp_stride=1, padding=0, dilation=1, bias=True, padding_mode='zeros'):\n",
    "  def Svertca(matrix):\n",
    "\n",
    "    #всегда 1 шаг\n",
    "    stride = 1\n",
    "\n",
    "    #добавление отступов и padding в входной матрице\n",
    "    pad = kernel_size - 1\n",
    "    result_matrix = []\n",
    "    for matr in matrix:\n",
    "      zero_tensor = np.zeros((((matr.shape[0]-1)*(transp_stride)+1), ((matr.shape[1]-1)*(transp_stride)+1)))\n",
    "      for a in range (0, zero_tensor.shape[0], transp_stride):\n",
    "        for b in range (0, zero_tensor.shape[1], transp_stride):\n",
    "          zero_tensor[a][b] = matr[a//(transp_stride)][b//(transp_stride)]\n",
    "\n",
    "      pad_matr = np.pad(zero_tensor, pad_width=pad, mode='constant')\n",
    "      result_matrix.append(pad_matr)\n",
    "    matrix = torch.tensor(result_matrix)\n",
    "\n",
    "    #генерация bias\n",
    "    if bias == True:\n",
    "      bias_val = torch.rand(out_channels)\n",
    "    else:\n",
    "      bias_val = torch.zeros(out_channels)\n",
    "\n",
    "    #padding_mode\n",
    "    if (padding_mode == 'zeros'):\n",
    "      pad = torch.nn.ZeroPad2d(padding)\n",
    "      matrix = pad(matrix)\n",
    "    if (padding_mode == 'reflect'):\n",
    "      pad = torch.nn.ReflectionPad2d(padding)\n",
    "      matrix = pad(matrix)\n",
    "    if (padding_mode == 'replicate'):\n",
    "      pad = torch.nn.ReplicationPad2d(padding)\n",
    "      matrix = pad(matrix)\n",
    "    if (padding_mode == 'circular'):\n",
    "      pad = torch.nn.CircularPad2d(padding)\n",
    "      matrix = pad(matrix)\n",
    "\n",
    "    #генерация ядра\n",
    "    filter = np.array(torch.rand(out_channels, in_channels, kernel_size, kernel_size))\n",
    "\n",
    "    #инвертирование ядра для ConvTranspose2d\n",
    "    filter_for_transpose = []\n",
    "    for j in range(out_channels):\n",
    "      filter_in = []\n",
    "      for i in range(in_channels):\n",
    "        filter_in.append(np.flip(np.array(filter[j][i])))\n",
    "      filter_for_transpose.append(filter_in)\n",
    "\n",
    "    filter_for_transpose = torch.tensor(filter_for_transpose)\n",
    "    filter_for_transpose = filter_for_transpose.reshape(in_channels, out_channels, kernel_size, kernel_size)\n",
    "\n",
    "\n",
    "\n",
    "    spisok = []\n",
    "    for l in range(out_channels):\n",
    "      feature_map = np.array([]) #генерация пустой feature-map\n",
    "      for i in range (0, matrix.shape[1]-((filter.shape[2]-1)*dilation+1)+1, stride): #(filter.size - 1)*dilation + 1 при delation\n",
    "        for j in range (0, matrix.shape[2]-((filter.shape[3]-1)*dilation+1)+1, stride):\n",
    "          summa = 0\n",
    "          for c in range (in_channels):\n",
    "            val = matrix[c][i:i+(filter.shape[2]-1)*dilation+1:dilation, j:j+(filter.shape[3]-1)*dilation+1:dilation]\n",
    "            mini_sum = (val*filter[l][c]).sum()\n",
    "            summa = summa + mini_sum\n",
    "          feature_map = np.append(feature_map, float(summa + bias_val[l])) #bias\n",
    "      spisok.append(feature_map.reshape((matrix.shape[1]-((filter.shape[2]-1)*dilation+1))//stride+1, (matrix.shape[2]-((filter.shape[3]-1)*dilation+1))//stride+1))\n",
    "\n",
    "\n",
    "    return np.array(spisok), torch.tensor(np.array(filter_for_transpose)), torch.tensor(np.array(bias_val))\n",
    "\n",
    "  return Svertca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "fbd729a1-61e0-4d87-b743-f61733d2b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "tensor1 = torch.rand(3, 5, 6)\n",
    "\n",
    "myFunction = myTranspConv2d(in_channels=3, out_channels=1, kernel_size=3, transp_stride=2, bias=True,)\n",
    "result, kernel, bias_val = myFunction(tensor1)\n",
    "torchFunction = torch.nn.ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=3, stride=2, bias=True,)\n",
    "torchFunction.weight.data = kernel\n",
    "torchFunction.bias.data = bias_val\n",
    "myResult = str(np.round(result, 2))\n",
    "torchResult = str(np.round(np.array(torchFunction(tensor1).data), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7fac8731-d269-4918-a149-f68b15d37758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[[0.83 0.99 1.52 1.6  2.25 1.57 2.36 1.54 2.18 1.11 1.65 1.41 1.58]\\n  [0.88 1.05 1.59 1.67 2.15 1.73 2.29 1.76 1.95 1.19 1.67 1.5  1.43]\\n  [1.14 1.33 2.7  1.74 3.51 2.11 3.96 1.87 3.58 2.08 3.73 2.01 2.51]\\n  [0.96 1.36 1.85 1.22 2.13 1.89 2.33 1.5  2.29 2.03 2.77 1.82 1.9 ]\\n  [2.52 2.3  3.81 1.31 3.87 2.85 4.31 2.01 4.49 2.42 5.05 2.33 3.25]\\n  [1.85 2.02 2.48 1.34 2.66 2.5  3.19 1.72 2.76 2.13 2.94 2.17 2.08]\\n  [2.79 2.61 4.23 1.83 4.99 2.66 5.19 2.1  4.33 2.03 3.78 1.97 2.02]\\n  [1.86 2.07 2.89 1.74 2.79 2.15 2.91 1.99 2.48 1.64 2.02 1.24 1.29]\\n  [1.99 1.59 3.18 2.07 3.78 1.84 4.47 2.76 4.95 1.85 4.05 2.42 2.72]\\n  [1.01 1.08 1.88 1.72 2.12 1.3  2.48 2.57 3.05 1.65 2.87 2.26 2.36]\\n  [1.07 0.82 1.65 1.11 1.62 0.93 2.67 1.38 2.93 0.97 2.95 1.23 1.53]]]'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "028e210d-a7aa-48d8-9368-e1a1e05c14df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[[0.83 0.99 1.52 1.6  2.25 1.57 2.36 1.54 2.18 1.11 1.65 1.41 1.58]\\n  [0.88 1.05 1.59 1.67 2.15 1.73 2.29 1.76 1.95 1.19 1.67 1.5  1.43]\\n  [1.14 1.33 2.7  1.74 3.51 2.11 3.96 1.87 3.58 2.08 3.73 2.01 2.51]\\n  [0.96 1.36 1.85 1.22 2.13 1.89 2.33 1.5  2.29 2.03 2.77 1.82 1.9 ]\\n  [2.52 2.3  3.81 1.31 3.87 2.85 4.31 2.01 4.49 2.42 5.05 2.33 3.25]\\n  [1.85 2.02 2.48 1.34 2.66 2.5  3.19 1.72 2.76 2.13 2.94 2.17 2.08]\\n  [2.79 2.61 4.23 1.83 4.99 2.66 5.19 2.1  4.33 2.03 3.78 1.97 2.02]\\n  [1.86 2.07 2.89 1.74 2.79 2.15 2.91 1.99 2.48 1.64 2.02 1.24 1.29]\\n  [1.99 1.59 3.18 2.07 3.78 1.84 4.47 2.76 4.95 1.85 4.05 2.42 2.72]\\n  [1.01 1.08 1.88 1.72 2.12 1.3  2.48 2.57 3.05 1.65 2.87 2.26 2.36]\\n  [1.07 0.82 1.65 1.11 1.62 0.93 2.67 1.38 2.93 0.97 2.95 1.23 1.53]]]'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b634e78-765f-40cc-80b7-fd3f46062882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neyronki",
   "language": "python",
   "name": "neyronki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
