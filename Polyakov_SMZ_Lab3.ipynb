{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9526707e-c13e-41b9-8d6a-c0e289bbe035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "class BloatWareConvTranspose2D():\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, dtype=None):\n",
    "        \n",
    "        if isinstance(in_channels, int) and in_channels > 0:\n",
    "            self.in_channels = in_channels\n",
    "        else:\n",
    "            raise ValueError(\"Invalid in_channels\")\n",
    "        \n",
    "        if isinstance(out_channels, int) and out_channels > 0:\n",
    "            self.out_channels = out_channels\n",
    "        else:\n",
    "            raise ValueError(\"Invalid out_channels\")\n",
    "        \n",
    "        if isinstance(groups, int) and groups > 0:\n",
    "            self.groups = groups\n",
    "        else:\n",
    "            raise ValueError(\"Invalid groups\")\n",
    "\n",
    "        if isinstance(stride, int) and stride > 0:\n",
    "            self.stride = stride\n",
    "        else:\n",
    "            raise ValueError(\"Invalid stride\")\n",
    "        \n",
    "        if isinstance(padding, int) and padding > -1:\n",
    "            self.padding = padding\n",
    "        else:\n",
    "            raise ValueError(\"Invalid padding\")\n",
    "            \n",
    "        if isinstance(output_padding, int) and output_padding > -1:\n",
    "            self.output_padding = output_padding\n",
    "        else:\n",
    "            raise ValueError(\"Invalid output_padding\")\n",
    "            \n",
    "        if isinstance(dilation, int) and dilation > 0:\n",
    "            self.dilation = dilation\n",
    "        else:\n",
    "            raise ValueError(\"Invalid dilation\")\n",
    "            \n",
    "        if not((self.in_channels % self.groups == 0) and (self.out_channels % self.groups == 0)):\n",
    "            raise ValueError(\"in_channels and out_channels must both be divisible by groups\")\n",
    "        \n",
    "        if (self.output_padding >= self.dilation and self.output_padding >= self.stride) or (self.output_padding >= self.dilation and self.output_padding >= self.stride):\n",
    "            raise ValueError(\"output_padding should be smaller than dilation or stride\")\n",
    "\n",
    "        if bias == True:\n",
    "          self.bias = torch.rand(self.out_channels)\n",
    "        else:\n",
    "          self.bias = torch.zeros(self.out_channels)\n",
    "    \n",
    "        if isinstance(kernel_size, int):\n",
    "            self.weight = torch.rand(\n",
    "                self.in_channels,\n",
    "                self.out_channels,\n",
    "                kernel_size,\n",
    "                kernel_size,\n",
    "            )\n",
    "            self.kernel_size = kernel_size\n",
    "        else:\n",
    "            raise ValueError(\"kernel size must be int or tuple\")\n",
    "\n",
    "        self.dtype = dtype\n",
    "    \n",
    "    def forward(self, input_tensor):\n",
    "        result = []\n",
    "\n",
    "        for l in range(self.out_channels):\n",
    "    \n",
    "          feature_map = torch.zeros((input_tensor.shape[1]-1)*self.stride + self.dilation * (self.kernel_size-1)+1, (input_tensor.shape[2]-1)*self.stride  + self.dilation * (self.kernel_size-1)+1 ) #генерация пустой feature-map\n",
    "          for c in range (self.in_channels):\n",
    "    \n",
    "            for i in range (0, input_tensor.shape[1]):  #проход по всем пикселям изображения\n",
    "              for j in range (0, input_tensor.shape[2]):\n",
    "    \n",
    "                val = input_tensor[c][i][j]\n",
    "                proizv = val*self.weight[c][l]\n",
    "    \n",
    "                zero_tensor = torch.zeros((self.weight.shape[2]-1)*self.dilation+1, (self.weight.shape[3]-1)*self.dilation+1)\n",
    "    \n",
    "                for a in range (0, zero_tensor.shape[0], self.dilation):\n",
    "                  for b in range (0, zero_tensor.shape[1], self.dilation):\n",
    "                    zero_tensor[a][b] = proizv[a//self.dilation][b//self.dilation]\n",
    "    \n",
    "                res = np.add((zero_tensor), feature_map[i*self.stride:i*self.stride+(self.weight.shape[2]-1)*self.dilation+1, j*self.stride:j*self.stride+(self.weight.shape[3]-1)*self.dilation+1])\n",
    "                feature_map[i*self.stride:i*self.stride+(self.weight.shape[2]-1)*self.dilation+1, j*self.stride:j*self.stride+(self.weight.shape[3]-1)*self.dilation+1] = res\n",
    "    \n",
    "    \n",
    "          result.append(np.add(feature_map, np.full((feature_map.shape), self.bias[l])))\n",
    "    \n",
    "    \n",
    "        for t in range(len(result)):\n",
    "          if self.output_padding > 0:\n",
    "            pad_func = torch.nn.ConstantPad1d((0, self.output_padding, 0, self.output_padding), 0)\n",
    "            result[t] = pad_func(result[t])\n",
    "    \n",
    "          result[t] = result[t][0+self.padding:result[t].shape[0]-self.padding, 0+self.padding:result[t].shape[1]-self.padding]\n",
    "\n",
    "        return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fde7dab-2430-42cc-b350-7635b35ddbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchConvTranspose2D = torch.nn.ConvTranspose2d(15, 5, 3, stride=1, padding=1, dilation=1)\n",
    "input_image = torch.randn(15, 50, 50)\n",
    "output = torchConvTranspose2D(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16474b3d-b96f-4e76-9336-7cac7c3f5ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "myConvTranspose2D = BloatWareConvTranspose2D(15, 5, 3, stride=1, padding=1, dilation=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e17ce15d-53cf-46d0-9746-97fbc7d9e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "myConvTranspose2D.weight = torchConvTranspose2D.weight.detach().numpy()\n",
    "myConvTranspose2D.bias = torchConvTranspose2D.bias.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e0bbbeb-41d2-4567-b608-4f0cb8f8a590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchConvTranspose2D.weight.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a29ff7d8-4cf8-42be-93b0-455344316d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mock = myConvTranspose2D.forward(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da349a4f-cd9d-4a02-8d06-00f90d08d274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.32806575,  0.19140458, -0.38019127, ...,  0.03490135,\n",
       "          0.49552664,  0.62894523],\n",
       "        [ 0.29940262,  0.04433158,  0.45483455, ...,  0.289754  ,\n",
       "         -0.11334548, -0.3289992 ],\n",
       "        [-1.5907145 , -2.0165308 ,  0.7746327 , ..., -0.73511493,\n",
       "         -0.03588326, -1.4044667 ],\n",
       "        ...,\n",
       "        [ 0.34540305,  0.16014487,  0.38106743, ..., -0.2564003 ,\n",
       "         -0.389469  , -1.100019  ],\n",
       "        [ 0.2312895 ,  0.93495333,  0.2270971 , ..., -0.47608882,\n",
       "          0.66813123, -0.1007289 ],\n",
       "        [-0.20820044,  0.17850864,  0.3281487 , ..., -0.32019198,\n",
       "          0.7752029 , -1.0750511 ]],\n",
       "\n",
       "       [[-0.16106942,  0.87301356, -0.98776275, ...,  0.96094114,\n",
       "          0.7396459 , -0.97358143],\n",
       "        [-0.62295705, -0.90292037, -1.30724   , ...,  1.459812  ,\n",
       "         -1.898841  , -1.3842762 ],\n",
       "        [ 1.5632981 ,  1.152181  ,  1.3168902 , ...,  0.373163  ,\n",
       "         -1.5213273 ,  0.62232584],\n",
       "        ...,\n",
       "        [-0.5605943 , -0.3837295 , -0.9478675 , ...,  1.3187928 ,\n",
       "         -0.7962604 , -1.7036341 ],\n",
       "        [-0.6500331 , -0.8411394 , -1.301807  , ..., -2.0380144 ,\n",
       "         -1.2879795 ,  1.0633188 ],\n",
       "        [-0.13555259, -0.04961753,  1.6877657 , ..., -0.06672496,\n",
       "         -0.932421  ,  0.80236495]],\n",
       "\n",
       "       [[-0.76410186, -0.01141606, -0.9982455 , ...,  1.4992931 ,\n",
       "          0.8773694 ,  0.7354487 ],\n",
       "        [ 0.05688504,  0.8395659 , -0.05466045, ..., -1.6940461 ,\n",
       "          0.48385194,  0.36536074],\n",
       "        [ 0.4903826 , -0.24310094,  0.13200201, ...,  0.35657954,\n",
       "         -0.98416066,  0.4140005 ],\n",
       "        ...,\n",
       "        [ 0.20438276,  0.0897218 ,  0.24506542, ..., -0.10609895,\n",
       "         -0.8735956 , -1.8145901 ],\n",
       "        [-0.5104754 ,  0.12843561,  1.8590887 , ...,  0.1922619 ,\n",
       "         -0.45217714,  1.6267287 ],\n",
       "        [ 0.8127949 ,  0.16721681, -0.1538854 , ...,  0.6296338 ,\n",
       "         -0.20539206,  1.1399612 ]],\n",
       "\n",
       "       [[ 1.7167189 , -0.5581492 , -2.213138  , ...,  0.17014396,\n",
       "          0.43484268,  0.34145913],\n",
       "        [-0.620898  ,  0.57904136,  1.7502786 , ...,  1.2980207 ,\n",
       "          0.09540993, -1.0546491 ],\n",
       "        [-1.3597517 ,  1.4122318 , -1.0188297 , ..., -1.2443851 ,\n",
       "         -0.9335369 ,  0.39180174],\n",
       "        ...,\n",
       "        [-0.2177841 , -1.0565431 ,  0.04503269, ...,  1.821003  ,\n",
       "         -0.73117864, -0.82163846],\n",
       "        [ 1.2752873 , -0.526252  , -2.4683027 , ..., -1.2099531 ,\n",
       "         -0.83316183,  0.7969035 ],\n",
       "        [-0.21877185, -0.02571489, -1.0052922 , ..., -0.6180967 ,\n",
       "         -1.590447  ,  0.5143161 ]],\n",
       "\n",
       "       [[ 0.20339245,  0.72144705, -0.52792317, ...,  0.66886115,\n",
       "         -0.8477746 , -0.2252397 ],\n",
       "        [-0.76448065,  0.45473862,  0.73359567, ...,  0.259611  ,\n",
       "         -1.0279073 ,  0.7393539 ],\n",
       "        [ 1.3976165 , -0.8070787 ,  0.03203151, ..., -0.6060701 ,\n",
       "         -0.1362137 ,  0.6992292 ],\n",
       "        ...,\n",
       "        [-0.6954561 , -0.16776681, -1.8433503 , ...,  1.3522543 ,\n",
       "         -0.4222499 ,  0.15979506],\n",
       "        [-0.14066552, -0.24352404, -0.45504007, ...,  0.91456956,\n",
       "          0.89826685, -0.7780425 ],\n",
       "        [-0.04401644,  0.33749515,  1.1065723 , ...,  0.05137448,\n",
       "         -0.21044113, -0.12608401]]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08cb2ac4-1c35-434e-803c-1fe27beec5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3281,  0.1914, -0.3802,  ...,  0.0349,  0.4955,  0.6289],\n",
       "         [ 0.2994,  0.0443,  0.4548,  ...,  0.2898, -0.1133, -0.3290],\n",
       "         [-1.5907, -2.0165,  0.7746,  ..., -0.7351, -0.0359, -1.4045],\n",
       "         ...,\n",
       "         [ 0.3454,  0.1601,  0.3811,  ..., -0.2564, -0.3895, -1.1000],\n",
       "         [ 0.2313,  0.9350,  0.2271,  ..., -0.4761,  0.6681, -0.1007],\n",
       "         [-0.2082,  0.1785,  0.3281,  ..., -0.3202,  0.7752, -1.0751]],\n",
       "\n",
       "        [[-0.1611,  0.8730, -0.9878,  ...,  0.9609,  0.7396, -0.9736],\n",
       "         [-0.6230, -0.9029, -1.3072,  ...,  1.4598, -1.8988, -1.3843],\n",
       "         [ 1.5633,  1.1522,  1.3169,  ...,  0.3732, -1.5213,  0.6223],\n",
       "         ...,\n",
       "         [-0.5606, -0.3837, -0.9479,  ...,  1.3188, -0.7963, -1.7036],\n",
       "         [-0.6500, -0.8411, -1.3018,  ..., -2.0380, -1.2880,  1.0633],\n",
       "         [-0.1356, -0.0496,  1.6878,  ..., -0.0667, -0.9324,  0.8024]],\n",
       "\n",
       "        [[-0.7641, -0.0114, -0.9982,  ...,  1.4993,  0.8774,  0.7354],\n",
       "         [ 0.0569,  0.8396, -0.0547,  ..., -1.6940,  0.4839,  0.3654],\n",
       "         [ 0.4904, -0.2431,  0.1320,  ...,  0.3566, -0.9842,  0.4140],\n",
       "         ...,\n",
       "         [ 0.2044,  0.0897,  0.2451,  ..., -0.1061, -0.8736, -1.8146],\n",
       "         [-0.5105,  0.1284,  1.8591,  ...,  0.1923, -0.4522,  1.6267],\n",
       "         [ 0.8128,  0.1672, -0.1539,  ...,  0.6296, -0.2054,  1.1400]],\n",
       "\n",
       "        [[ 1.7167, -0.5581, -2.2131,  ...,  0.1701,  0.4348,  0.3415],\n",
       "         [-0.6209,  0.5790,  1.7503,  ...,  1.2980,  0.0954, -1.0546],\n",
       "         [-1.3598,  1.4122, -1.0188,  ..., -1.2444, -0.9335,  0.3918],\n",
       "         ...,\n",
       "         [-0.2178, -1.0565,  0.0450,  ...,  1.8210, -0.7312, -0.8216],\n",
       "         [ 1.2753, -0.5263, -2.4683,  ..., -1.2100, -0.8332,  0.7969],\n",
       "         [-0.2188, -0.0257, -1.0053,  ..., -0.6181, -1.5904,  0.5143]],\n",
       "\n",
       "        [[ 0.2034,  0.7214, -0.5279,  ...,  0.6689, -0.8478, -0.2252],\n",
       "         [-0.7645,  0.4547,  0.7336,  ...,  0.2596, -1.0279,  0.7394],\n",
       "         [ 1.3976, -0.8071,  0.0320,  ..., -0.6061, -0.1362,  0.6992],\n",
       "         ...,\n",
       "         [-0.6955, -0.1678, -1.8434,  ...,  1.3523, -0.4223,  0.1598],\n",
       "         [-0.1407, -0.2435, -0.4550,  ...,  0.9146,  0.8983, -0.7780],\n",
       "         [-0.0440,  0.3375,  1.1066,  ...,  0.0514, -0.2104, -0.1261]]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f0996b2-c3e2-4d7b-a56a-91dbced6d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = output.detach().numpy().astype(\"float16\") == output_mock.astype(\n",
    "    \"float16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "250ef4e3-da99-4a24-a342-84384435b2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59dc7d99-6609-4390-b8c3-0ea456ff4dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTranspConv2d(matrix, in_channels, out_channels, kernel_size, transp_stride=1, padding=0, dilation=1, bias=True, padding_mode='zeros'):\n",
    "    stride = 1\n",
    "\n",
    "    #добавление отступов и padding в входной матрице\n",
    "    pad = kernel_size - 1\n",
    "    result_matrix = []\n",
    "    for matr in matrix:\n",
    "      zero_tensor = np.zeros((((matr.shape[0]-1)*(transp_stride)+1), ((matr.shape[1]-1)*(transp_stride)+1)))\n",
    "      for a in range (0, zero_tensor.shape[0], transp_stride):\n",
    "        for b in range (0, zero_tensor.shape[1], transp_stride):\n",
    "          zero_tensor[a][b] = matr[a//(transp_stride)][b//(transp_stride)]\n",
    "\n",
    "      pad_matr = np.pad(zero_tensor, pad_width=pad, mode='constant')\n",
    "      result_matrix.append(pad_matr)\n",
    "    matrix = torch.tensor(result_matrix)\n",
    "\n",
    "    #генерация bias\n",
    "    if bias == True:\n",
    "      bias_val = torch.rand(out_channels)\n",
    "    else:\n",
    "      bias_val = torch.zeros(out_channels)\n",
    "\n",
    "    #padding_mode\n",
    "    if (padding_mode == 'zeros'):\n",
    "      pad = torch.nn.ZeroPad2d(padding)\n",
    "      matrix = pad(matrix)\n",
    "    if (padding_mode == 'reflect'):\n",
    "      pad = torch.nn.ReflectionPad2d(padding)\n",
    "      matrix = pad(matrix)\n",
    "    if (padding_mode == 'replicate'):\n",
    "      pad = torch.nn.ReplicationPad2d(padding)\n",
    "      matrix = pad(matrix)\n",
    "    if (padding_mode == 'circular'):\n",
    "      pad = torch.nn.CircularPad2d(padding)\n",
    "      matrix = pad(matrix)\n",
    "\n",
    "    #генерация ядра\n",
    "    filter = np.array(torch.rand(out_channels, in_channels, kernel_size, kernel_size))\n",
    "\n",
    "    #инвертирование ядра для ConvTranspose2d\n",
    "    filter_for_transpose = []\n",
    "    for j in range(out_channels):\n",
    "      filter_in = []\n",
    "      for i in range(in_channels):\n",
    "        filter_in.append(np.flip(np.array(filter[j][i])))\n",
    "      filter_for_transpose.append(filter_in)\n",
    "\n",
    "    filter_for_transpose = torch.tensor(filter_for_transpose)\n",
    "    filter_for_transpose = filter_for_transpose.reshape(in_channels, out_channels, kernel_size, kernel_size)\n",
    "\n",
    "\n",
    "\n",
    "    result = []\n",
    "    for l in range(out_channels):\n",
    "      feature_map = np.array([]) #генерация пустой feature-map\n",
    "      for i in range (0, matrix.shape[1]-((filter.shape[2]-1)*dilation+1)+1, stride): #(filter.size - 1)*dilation + 1 при delation\n",
    "        for j in range (0, matrix.shape[2]-((filter.shape[3]-1)*dilation+1)+1, stride):\n",
    "          summa = 0\n",
    "          for c in range (in_channels):\n",
    "            val = matrix[c][i:i+(filter.shape[2]-1)*dilation+1:dilation, j:j+(filter.shape[3]-1)*dilation+1:dilation]\n",
    "            mini_sum = (val*filter[l][c]).sum()\n",
    "            summa = summa + mini_sum\n",
    "          feature_map = np.append(feature_map, float(summa + bias_val[l])) #bias\n",
    "      result.append(feature_map.reshape((matrix.shape[1]-((filter.shape[2]-1)*dilation+1))//stride+1, (matrix.shape[2]-((filter.shape[3]-1)*dilation+1))//stride+1))\n",
    "\n",
    "\n",
    "    return np.array(result), torch.tensor(np.array(filter_for_transpose)), torch.tensor(np.array(bias_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbd729a1-61e0-4d87-b743-f61733d2b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "tensor1 = torch.rand(3, 5, 6)\n",
    "\n",
    "result, kernel, bias_val = myTranspConv2d(tensor1, in_channels=3, out_channels=1, kernel_size=3, transp_stride=2, bias=True,)\n",
    "torchFunction = torch.nn.ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=3, stride=2, bias=True,)\n",
    "torchFunction.weight.data = kernel\n",
    "torchFunction.bias.data = bias_val\n",
    "myResult = str(np.round(result, 2))\n",
    "torchResult = str(np.round(np.array(torchFunction(tensor1).data), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fac8731-d269-4918-a149-f68b15d37758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchFunction(tensor1).detach().numpy().astype(\"float16\") == result.astype(\n",
    "    \"float16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b634e78-765f-40cc-80b7-fd3f46062882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neyronki",
   "language": "python",
   "name": "neyronki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
